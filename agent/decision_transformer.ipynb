{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.0.0-cp310-none-macosx_11_0_arm64.whl (55.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hCollecting networkx\n",
      "  Downloading networkx-3.1-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting jinja2\n",
      "  Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "Collecting sympy\n",
      "  Using cached sympy-1.11.1-py3-none-any.whl (6.5 MB)\n",
      "Requirement already satisfied: filelock in /Users/clarec/miniconda3/lib/python3.10/site-packages (from torch) (3.11.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/clarec/miniconda3/lib/python3.10/site-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/clarec/miniconda3/lib/python3.10/site-packages (from jinja2->torch) (2.1.2)\n",
      "Collecting mpmath>=0.19\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, sympy, networkx, jinja2, torch\n",
      "Successfully installed jinja2-3.1.2 mpmath-1.3.0 networkx-3.1 sympy-1.11.1 torch-2.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class DecisionTransformer(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, reward_dim, embedding_dim, nhead, num_layers):\n",
    "        super(DecisionTransformer, self).__init__() \n",
    "        self.state_embedding = nn.Linear(state_dim, embedding_dim) \n",
    "        self.action_embedding = nn.Linear(action_dim, embedding_dim) \n",
    "        self.reward_embedding = nn.Linear(reward_dim, embedding_dim)\n",
    "        \n",
    "        self.transformer = nn.Transformer(embedding_dim, nhead, num_layers) \n",
    "        self.output_layer = nn.Linear(embedding_dim, action_dim)\n",
    "        \n",
    "    def forward(self, states, actions, rewards, mask=None): \n",
    "        state_embeds = self.state_embedding(states) \n",
    "        action_embeds = self.action_embedding(actions) \n",
    "        reward_embeds = self.reward_embedding(rewards)\n",
    "        input_embeds = torch.cat((state_embeds, action_embeds, reward_embeds), dim=1) \n",
    "        # tgt = torch.rand((20, 32, 512))\n",
    "        # TODO: what is tgt?\n",
    "        transformer_output = self.transformer(input_embeds.transpose(0, 1), src_key_padding_mask=mask, tgt=input_embeds.transpose(0, 1)) \n",
    "        action_logits = self.output_layer(transformer_output[-1])\n",
    "        return action_logits\n",
    "\n",
    "state_dim = 10 \n",
    "action_dim = 5 \n",
    "reward_dim = 1 \n",
    "embedding_dim = 64 \n",
    "nhead = 4 \n",
    "num_layers = 2\n",
    "\n",
    "model = DecisionTransformer(state_dim, action_dim, reward_dim, embedding_dim, nhead, num_layers)\n",
    "\n",
    "# Load dataset\n",
    "states = torch.randn(32, 10, state_dim)\n",
    "actions = torch.randint(0, action_dim, (32, 10, 1))\n",
    "rewards = torch.randn(32, 10, 1)\n",
    "\n",
    "# Transfer to one-hot\n",
    "actions_one_hot = torch.zeros(32, 10, action_dim) \n",
    "actions_one_hot.scatter_(2, actions, 1)\n",
    "\n",
    "# Got outputs of the model\n",
    "action_logits = model(states, actions_one_hot, rewards)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# Training (one batch)\n",
    "optimizer.zero_grad()\n",
    "loss = loss_fn(action_logits, actions.squeeze(-1)[:, -1]) \n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5293,  0.0439,  0.8802, -0.7718, -1.1242],\n",
       "        [ 0.1930,  0.4248,  1.2229, -0.2353, -0.4624],\n",
       "        [-0.1271,  0.2617,  0.8487, -0.2455, -0.5983],\n",
       "        [-0.5104, -0.2908,  1.0161, -0.5251, -0.6145],\n",
       "        [-0.2598,  0.1549,  1.0200, -0.4023, -0.6141],\n",
       "        [-0.0993,  0.8159,  1.1459, -0.6534, -0.3159],\n",
       "        [-0.8067,  0.1132,  0.4998, -0.0729, -1.0338],\n",
       "        [-0.0929,  0.1405,  1.0532, -0.5955, -0.9101],\n",
       "        [ 0.0291,  0.2613,  0.8143, -0.2087, -0.1612],\n",
       "        [-0.0151,  0.2863,  0.4164, -0.5595, -0.3650],\n",
       "        [-0.5437,  0.4898,  0.4624, -0.4046, -0.8115],\n",
       "        [ 0.1059,  0.3046,  0.8754, -0.5264, -0.6838],\n",
       "        [-0.0451, -0.2717,  0.1272, -0.0477, -1.1460],\n",
       "        [-0.0667,  0.1108,  0.4717, -0.1777, -1.0654],\n",
       "        [-0.0175,  0.5096,  0.7076, -0.5101, -0.1396],\n",
       "        [ 0.0511,  0.5153,  1.2029, -0.3754, -1.0642],\n",
       "        [-0.2405,  0.2329,  1.1951, -0.1411, -1.0951],\n",
       "        [-0.0241,  0.3872,  1.2932, -0.4436, -0.5085],\n",
       "        [ 0.3571,  0.6147,  0.6032, -0.5892, -0.5580],\n",
       "        [-0.1619,  0.3144,  0.7290,  0.0044, -0.6804],\n",
       "        [ 0.0146,  0.3747,  0.9623, -0.4856, -1.1597],\n",
       "        [ 0.3024,  0.4389,  0.8408,  0.0315, -0.5857],\n",
       "        [-0.3282,  0.3494,  1.0597, -0.5083, -0.6759],\n",
       "        [ 0.1210,  0.4967,  0.9064, -0.5393, -0.3703],\n",
       "        [-0.1940,  0.4784,  0.5727, -0.5823, -0.6250],\n",
       "        [-0.2181,  0.2797,  0.6947, -0.6178, -1.1476],\n",
       "        [-0.4316,  0.1431,  0.8540, -0.6760, -0.7250],\n",
       "        [-0.1189, -0.1399,  0.5686, -0.0927, -0.4199],\n",
       "        [-0.4927,  0.1440,  1.0332, -0.6389, -0.8879],\n",
       "        [-0.0345, -0.0088,  0.5459, -0.8088, -1.1897],\n",
       "        [-0.5492,  0.2596,  0.9026, -0.0863, -0.6876],\n",
       "        [-0.1504,  0.3640,  0.9205, -0.4572, -0.6694]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_logits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
